{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "utility-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-decimal",
   "metadata": {},
   "source": [
    "## Ensamble de modelos y *random forest*\n",
    "\n",
    "En un ensamble de modelos combinamos varios modelos para mejorar las predicciones (clasificación o regresión) de los modelos individuales.\n",
    "\n",
    "Un grupo de modelos así se llama un **ensamble de modelos** y la técnica se llama *ensemble learning*.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "* Se puede entrenar un grupo de árboles de decisión, cada uno en un subconjunto aleatorio del conjunto de entrenamiento.\n",
    "* Para realizar predicciones, obtenemos las predicciones de cada árbol y la predicción final es la clase que obtiene la mayoría de los votos.\n",
    "* Este ensamble de modelos se llama un *random forest*.\n",
    "* Aunque es un método simple, es uno de los algoritmos más poderosos de *machine learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-fifty",
   "metadata": {},
   "source": [
    "### Clasificadores con votos\n",
    "\n",
    "| ![](figures_ensamble/fig7-1.png) |\n",
    "|----------------------------------|\n",
    "| Un grupo de clasificadores diversos. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-chancellor",
   "metadata": {},
   "source": [
    "| ![](figures_ensamble/fig7-2.png) |\n",
    "|----------------------------------|\n",
    "| Clasificador por votos (*hard voting*) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-viking",
   "metadata": {},
   "source": [
    "Un clasificador por votos a veces logra un *accuracy* mayor que el mejor clasificador del ensamble!\n",
    "\n",
    "De hecho, incluso si cada clasificador es **débil** (es solo ligeramente mejor que adivinanzas al azar), el ensamble puede ser un clasificador **fuerte** (si hay muchos clasificadores y son muy diversos).\n",
    "\n",
    "Por ejemplo: $1000$ clasificadores que son correctos 51% del tiempo. Usando la clase votado por la mayoría se puede esperar un rendimiento de 75%.\n",
    "\n",
    "Este es verdad sólo si todos los clasificadores son independientes, y cometen errores sin correlaciones. Pero ya que están entrenados usando los mismos datos habrán correlaciones. Así que el rendimiento será un poco menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numerous-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dense-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boxed-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "thick-gather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                          ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "                              voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thorough-maintenance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-spirituality",
   "metadata": {},
   "source": [
    "Si los clasificadores pueden dar la probabilidad para las clases (tienen un método `predict_proba()`) se puede predecir la clase con la mayor probabilidad, promediada sobre los clasificadores individuales. Este se llama *soft voting*.\n",
    "\n",
    "Para implementar este, hay que usar `voting='soft'` en `VotingClassifier`, y todos los clasificadores tienen que tener un método de `predict_proba()`.\n",
    "\n",
    "Por defecto SVC no tiene `predict_proba()`, pero se puede elegir la opción `True` para el hiperparámetro `probability` de ese modelo.\n",
    "\n",
    "**Ejercicio**: implementar *soft voting* en el modelo arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-guard",
   "metadata": {},
   "source": [
    "### *Bagging* y *pasting*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-computer",
   "metadata": {},
   "source": [
    "Otro método de tener un grupo de diverso de predictores es usar el mismo algoritmo para cada clasificador, pero usando distintos subconjuntos de entrenamiento.\n",
    "\n",
    "* Si reemplazamos las instancias en el conjunto total cada vez que está elegida para un subconjunto (es decir, la misma instancia puede aparecer varias veces en el subconjunto) se llama *bagging* (*bootstrap aggregating*).\n",
    "* Si **no** reemplazamos (cada instancia aparece solamente una vez en el subconjunto) se llama *pasting*.\n",
    "\n",
    "![](figures_ensamble/fig7-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-cosmetic",
   "metadata": {},
   "source": [
    "Con todos los predictores entrenados, agregamos las predicciones para llegar a la predicción del ensamble para una nueva instancia.\n",
    "\n",
    "* Para clasificación la función de agregación es típicamente el **modo estadístico** (la predicción más frecuente).\n",
    "* Para regresión es típicamente el promedio.\n",
    "\n",
    "Ya que usamos subconjuntos de entrenamiento, el *bias* es mayor para cada predictor individual, pero la agregación reduce el *bias* y la varianza.\n",
    "\n",
    "Es muy fácil parallelizar la operación de un ensamble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-sphere",
   "metadata": {},
   "source": [
    "##### Bagging/pasting en Scikit-Learn\n",
    "\n",
    "En Scikit-Learn se puede usar *bagging/pasting* muy facilmente. El código abajo es para entrenar 500 clasificadores de árbol de decisión, con 100 instancias elegidas aleatoriamente del conjunto de entrenamiento para cada árbol.\n",
    "\n",
    "La elección de las instancias es con reemplazo (*bagging*). Se puede cambiar a *pasting* con `bootstrap=False`.\n",
    "\n",
    "También se puede usar un valor entre $0.0$ y $1.0$ para `max_samples`, y multiplicará el número total de instancias de entrenamiento por ese valor.\n",
    "\n",
    "`n_jobs` es el número de núcleos (*cores*) de CPU usado. El valor `-1` indica que queremos usar todos los *cores* disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "solar-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            max_samples=100, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "precious-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-signature",
   "metadata": {},
   "source": [
    "`BaggingClassifier` implementa *soft voting* automaticamente si el clasificador de base puede estimar probabilidades de las clases (tiene el método `predict_proba()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-lebanon",
   "metadata": {},
   "source": [
    "| ![](figures_ensamble/fig7-5.png) |\n",
    "|----------------------------------|\n",
    "| Árbol de decisión vs. ensamble de 500 árboles. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-hollywood",
   "metadata": {},
   "source": [
    "El ensamble tiene un *bias* similar al árbol individual, pero hay menos varianza (comete el mismo número de errores en los datos de entrenamiento, pero el límite de decisión es menos irregular)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-economy",
   "metadata": {},
   "source": [
    "##### Evaluación *out-of-bag*\n",
    "\n",
    "Con *bagging* es posible tener la misma instancia usada varias veces en el entrenamiento de un predictor, mientras para otro no está usada ni una vez.\n",
    "\n",
    "![](figures_ensamble/fig_wiki_oob.jpg)\n",
    "\n",
    "Por defect, `BaggingClassifier` toma $m$ instancias del conjunto de entrenamiento, con reemplazo (`bootstrap=True`), donde $m$ es el tamaño del conjunto.\n",
    "\n",
    "$\\sim 63$% de las instancias están usadas, en promedio, para cada predictor. Los demás $37$% se llaman instancias *out-of-bag* (oob).\n",
    "\n",
    "Cada predictor tendrá un conjunto de instancias \"oob\" diferente.\n",
    "\n",
    "Podemos usar las instancias \"oob\" para evaluación del modelo, en vez de usar un conjunto de validación, con `oob_score=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "immediate-accounting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "phantom-peripheral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-floating",
   "metadata": {},
   "source": [
    "Esperamos un *accuracy* de 90.1% en el conjunto de preuba..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "revolutionary-relief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-associate",
   "metadata": {},
   "source": [
    "También se puede obtener la función de decisión \"oob\" para cada instancia de entrenamiento.\n",
    "\n",
    "En este caso el clasificador de base (`DecisionTreeClassifier`) tiene un método `predict_proba()`, así que la función de decisión retorna las probabilidades de cada clase para cada instancia de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "southern-cycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38586957, 0.61413043],\n",
       "       [0.32571429, 0.67428571],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06818182, 0.93181818],\n",
       "       [0.36507937, 0.63492063],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98994975, 0.01005025],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.79781421, 0.20218579],\n",
       "       [0.01162791, 0.98837209],\n",
       "       [0.79545455, 0.20454545],\n",
       "       [0.78034682, 0.21965318],\n",
       "       [0.96511628, 0.03488372],\n",
       "       [0.05319149, 0.94680851],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98192771, 0.01807229],\n",
       "       [0.91397849, 0.08602151],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03296703, 0.96703297],\n",
       "       [0.31770833, 0.68229167],\n",
       "       [0.89583333, 0.10416667],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9704142 , 0.0295858 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.63387978, 0.36612022],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11351351, 0.88648649],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.36612022, 0.63387978],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24630542, 0.75369458],\n",
       "       [0.36363636, 0.63636364],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01117318, 0.98882682],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01704545, 0.98295455],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [0.89690722, 0.10309278],\n",
       "       [0.93258427, 0.06741573],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02531646, 0.97468354],\n",
       "       [0.9895288 , 0.0104712 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98863636, 0.01136364],\n",
       "       [0.77777778, 0.22222222],\n",
       "       [0.40322581, 0.59677419],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.69945355, 0.30054645],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9068323 , 0.0931677 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.55307263, 0.44692737],\n",
       "       [0.12637363, 0.87362637],\n",
       "       [0.70224719, 0.29775281],\n",
       "       [0.87567568, 0.12432432],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13207547, 0.86792453],\n",
       "       [0.92655367, 0.07344633],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05294118, 0.94705882],\n",
       "       [0.02072539, 0.97927461],\n",
       "       [0.33507853, 0.66492147],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.81603774, 0.18396226],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24064171, 0.75935829],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94545455, 0.05454545],\n",
       "       [0.76086957, 0.23913043],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18378378, 0.81621622],\n",
       "       [0.59042553, 0.40957447],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05389222, 0.94610778],\n",
       "       [0.40677966, 0.59322034],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00584795, 0.99415205],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20218579, 0.79781421],\n",
       "       [0.47894737, 0.52105263],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03743316, 0.96256684],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.28229665, 0.71770335],\n",
       "       [0.87234043, 0.12765957],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.77647059, 0.22352941],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0201005 , 0.9798995 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9744898 , 0.0255102 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00568182, 0.99431818],\n",
       "       [0.95263158, 0.04736842],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.02369668, 0.97630332],\n",
       "       [0.22033898, 0.77966102],\n",
       "       [0.95808383, 0.04191617],\n",
       "       [0.27604167, 0.72395833],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.74566474, 0.25433526],\n",
       "       [0.40740741, 0.59259259],\n",
       "       [0.43352601, 0.56647399],\n",
       "       [0.83695652, 0.16304348],\n",
       "       [0.96410256, 0.03589744],\n",
       "       [0.04371585, 0.95628415],\n",
       "       [0.81313131, 0.18686869],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01142857, 0.98857143],\n",
       "       [0.98901099, 0.01098901],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9625    , 0.0375    ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99502488, 0.00497512],\n",
       "       [0.        , 1.        ],\n",
       "       [0.39790576, 0.60209424],\n",
       "       [0.30285714, 0.69714286],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27932961, 0.72067039],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01      , 0.99      ],\n",
       "       [0.63212435, 0.36787565],\n",
       "       [0.94797688, 0.05202312],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99401198, 0.00598802],\n",
       "       [0.98901099, 0.01098901],\n",
       "       [0.99411765, 0.00588235],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0972973 , 0.9027027 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06185567, 0.93814433],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05102041, 0.94897959],\n",
       "       [0.99507389, 0.00492611],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.60555556, 0.39444444],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11515152, 0.88484848],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9556962 , 0.0443038 ],\n",
       "       [0.97969543, 0.02030457],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01123596, 0.98876404],\n",
       "       [0.        , 1.        ],\n",
       "       [0.45263158, 0.54736842],\n",
       "       [0.8547486 , 0.1452514 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99431818, 0.00568182],\n",
       "       [0.01621622, 0.98378378],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97297297, 0.02702703],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25252525, 0.74747475],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.80681818, 0.19318182],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.08152174, 0.91847826],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04123711, 0.95876289],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04040404, 0.95959596],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82352941, 0.17647059],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93292683, 0.06707317],\n",
       "       [0.98870056, 0.01129944],\n",
       "       [0.24117647, 0.75882353],\n",
       "       [0.20942408, 0.79057592],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.23563218, 0.76436782],\n",
       "       [0.96045198, 0.03954802],\n",
       "       [0.0195122 , 0.9804878 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98773006, 0.01226994],\n",
       "       [0.        , 1.        ],\n",
       "       [0.49132948, 0.50867052],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07526882, 0.92473118],\n",
       "       [0.12209302, 0.87790698],\n",
       "       [0.98351648, 0.01648352],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.40217391, 0.59782609],\n",
       "       [0.1091954 , 0.8908046 ],\n",
       "       [0.51612903, 0.48387097],\n",
       "       [0.57803468, 0.42196532],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.60103627, 0.39896373],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27428571, 0.72571429],\n",
       "       [0.81420765, 0.18579235],\n",
       "       [0.05586592, 0.94413408],\n",
       "       [1.        , 0.        ],\n",
       "       [0.79057592, 0.20942408],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13173653, 0.86826347],\n",
       "       [0.01111111, 0.98888889],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9244186 , 0.0755814 ],\n",
       "       [0.15841584, 0.84158416],\n",
       "       [0.94021739, 0.05978261],\n",
       "       [0.01036269, 0.98963731],\n",
       "       [0.6185567 , 0.3814433 ],\n",
       "       [0.08988764, 0.91011236],\n",
       "       [0.99447514, 0.00552486],\n",
       "       [0.84153005, 0.15846995],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92090395, 0.07909605],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29885057, 0.70114943],\n",
       "       [0.98369565, 0.01630435],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.86627907, 0.13372093],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77380952, 0.22619048],\n",
       "       [0.93296089, 0.06703911],\n",
       "       [1.        , 0.        ],\n",
       "       [0.65533981, 0.34466019],\n",
       "       [0.51758794, 0.48241206],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88826816, 0.11173184],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.8150289 , 0.1849711 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.72486772, 0.27513228],\n",
       "       [0.11538462, 0.88461538],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.234375  , 0.765625  ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.87078652, 0.12921348],\n",
       "       [0.83417085, 0.16582915],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01538462, 0.98461538],\n",
       "       [0.93939394, 0.06060606],\n",
       "       [0.93969849, 0.06030151],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5026738 , 0.4973262 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97765363, 0.02234637],\n",
       "       [0.03626943, 0.96373057],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97714286, 0.02285714],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08152174, 0.91847826],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03482587, 0.96517413],\n",
       "       [1.        , 0.        ],\n",
       "       [0.15706806, 0.84293194],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01734104, 0.98265896],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41836735, 0.58163265],\n",
       "       [0.08064516, 0.91935484],\n",
       "       [0.1957672 , 0.8042328 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9787234 , 0.0212766 ],\n",
       "       [0.20467836, 0.79532164],\n",
       "       [0.99386503, 0.00613497],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9281768 , 0.0718232 ],\n",
       "       [0.34054054, 0.65945946],\n",
       "       [0.99393939, 0.00606061],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02312139, 0.97687861],\n",
       "       [0.98773006, 0.01226994],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0483871 , 0.9516129 ],\n",
       "       [0.65591398, 0.34408602]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-citation",
   "metadata": {},
   "source": [
    "##### Parches aleatorios y sub-espacios aleatorios (random patches, random subspaces)\n",
    "\n",
    "Se puede elegir los *features* usados en un `BaggingClassifier` aleatoriamente también. Hay dos hiperparámetros:\n",
    "\n",
    "* `max_features`\n",
    "* `bootstrap_features`\n",
    "\n",
    "Estos son equivalentes a `max_samples` y `bootstrap` pero para los *features* en vez de las instancias.\n",
    "\n",
    "Entonces, cada predictor está entrenado con un subconjunto aleatorio diferente de los *features*. Este es útil cuando los datos tienen muchas dimensiones.\n",
    "\n",
    "* Parches aleatorios (*random patches*) corresponde a usar *bagging* para instancias y *features*\n",
    "* Subespacios aleatorios (*random subspaces*) corresponde a usar todas las instancias de entrenamiento pero aplicar *bagging* a los *features* (`bootstrap=False`, `max_samples=1.0`, `bootstrap_features=True`, `max_features=x` donde `x < 1.0`)\n",
    "\n",
    "Usando subconjuntos de *features* tenemos aún más diversidad de predictores, así que el *bias* aumenta y la varianza disminuye."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-shoulder",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Ya hemos usado un *random forest* (ensamble de árboles de decisión, usualmente ocupando *bagging* con `max_samples` igual al tamaño del conjunto de entrenamiento).\n",
    "\n",
    "En vez de usar `BaggingClassifier` tenemos `RandomForestClassifier` que es más optimizado (también hay `RandomForestRegressor` para regresión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "confirmed-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "specialized-empty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-print",
   "metadata": {},
   "source": [
    "En un *random forest* un nodo está dividido en cada árbol usando el mejor *feature* en un conjunto aleatorio de *features*.\n",
    "\n",
    "Así que un *random forest* tiene más aleatoriedad que un árbol de decisión individual $\\Rightarrow$ más *bias*, menos varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-active",
   "metadata": {},
   "source": [
    "#### Extra-Trees\n",
    "\n",
    "Se puede usar umbrales aleatorios para cada *feature* en la división de los nodos, en vez de usar el mejor umbral posible (como lo que pasa con un árbol de decisión normal).\n",
    "\n",
    "Un *random forest* así se llama un esamble de *extremely randomized trees* (*Extra-Trees*).\n",
    "\n",
    "En Scikit-Learn está disponible con `ExtraTreesClassifier`.\n",
    "\n",
    "Este modelo de nuevo aumenta el *bias* pero reduce la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-peeing",
   "metadata": {},
   "source": [
    "#### Importancia de los *features*\n",
    "\n",
    "Con un *random forest* se puede determinar la importancia relativa de cada *feature*.\n",
    "\n",
    "La importancia de un *feature* está dada por cuanto está reducido la **impureza** (en promedio, sobre todos los árboles del bosque) por nodos que utilizan ese *feature*.\n",
    "\n",
    "El promedio es un promedio ponderado, donde el peso de cada nodo es igual al número de instancias de entrenamiento asociado.\n",
    "\n",
    "Scikit-Learn calcula esta importancia automaticamente para cada *feature* después del entrenamiento, y normaliza los resultados tal que $\\sum_{i=0}^{n} I_i = 1$, donde $I_i$ es la importancia de *feature* $i$.\n",
    "\n",
    "Se puede acceder a esta información con la variable `feature_importances_`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-links",
   "metadata": {},
   "source": [
    "##### Ejemplo con los dígitos de MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "subtle-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "billion-vertical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "hourly-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.hot,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "intended-thesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(1, 0.0, 'No importante'),\n",
       " Text(1, 0.009791489757332336, 'Muy importante')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(rnd_clf.feature_importances_)\n",
    "\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['No importante', 'Muy importante'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-confirmation",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
