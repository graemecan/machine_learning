{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affected-letter",
   "metadata": {},
   "source": [
    "# Introducción a redes neuronales artificiales con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-private",
   "metadata": {},
   "source": [
    "Las redes neuronales artificiales (RNAs) aparecieron en la investigación en Inteligencia Artificial en el año 1943 en un artículo escrito por un neurofisiólogo y un matemático (McCulloch & Pitts 1943).\n",
    "\n",
    "Había mucho interés al principio, con predicciones que iban a desarrollar \"máquinas inteligentes\". Pero la realidad no fue así (las redes neuronales en esa época todavía eran bastante básicas). La comunidad se perdió interés en ~1960s.\n",
    "\n",
    "~1980s inventaron nuevas arquitecturas y métodos de entrenamiento, pero el progreso fue lento y en los 90 otros métodos fueron inventados (e.g. *support vector machines*). Se fue el interés de nuevo...\n",
    "\n",
    "Hoy en día, hay mucho más interés en RNAs. Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-joint",
   "metadata": {},
   "source": [
    "* Hay muchos datos para entrenar las redes neuronales, y frecuentemente superan los otros métodos en problemas grandes y complejos.\n",
    "* Hay mucho más poder computacional (GPUs).\n",
    "* Los algoritmos de entrenamiento han sido mejorados.\n",
    "* Algunas limitaciones de las RNAs no son tan graves (por ejemplo, es poco común encontrar un mínimo local en el entrenamiento, y si pasa, muchas veces están cercas al mínimo global).\n",
    "* Hoy en día es una tecnología **exponencial**: hay mucho interés, así que atraen investigadores y fondos, los resultados mejoran y genera más interés..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-nerve",
   "metadata": {},
   "source": [
    "##### Neuronas biológicas\n",
    "\n",
    "![](figures_intro_rna/fig10-1.png)\n",
    "\n",
    "Cada neurona es bastante simple, pero están organizadas en una red de billones de neuronas, cada una con conexiones a miles de otras.\n",
    "\n",
    "![](figures_intro_rna/fig10-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-corner",
   "metadata": {},
   "source": [
    "### Cálculos lógicos con neuronas\n",
    "\n",
    "El modelo de una neurona **artificial**, desarrollado por McCulloch y Pitts, es mucho más básico: tiene entradas binarias y una salida binaria. La neurona se activa cuando hay más que un cierto número de sus entradas activas.\n",
    "\n",
    "![](figures_intro_rna/fig10-3.png)\n",
    "\n",
    "* La primera red a la izquierda es la función de identidad: si neurona $A$ se activa, neurona $C$ se activa también.\n",
    "* La segunda red corresponde al operador lógico AND: neurona $C$ se activa solamente cuando ambas neuronas $A$ y $B$ se activan.\n",
    "* La tercera red corresponde al operador lógico OR: neurona $C$ se activa si $A$ o $B$ se activan.\n",
    "* Finalmente, si se puede tener conexiones que inhiben la actividad de una neurona, la cuarta red corresponde a $A$ AND NOT $B$: neurona $C$ se activa solamente si $A$ se activa y $B$ se apaga.\n",
    "\n",
    "En principio, podemos combinar redes así para calcular cualquier expresión lógica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-subscription",
   "metadata": {},
   "source": [
    "### El Perceptrón\n",
    "\n",
    "Inventado en 1957 por Frank Rosenblatt, basado en un modelo para una neurona que se llama **unidad lógica de umbral** (*threshold logic unit*, TLU).\n",
    "\n",
    "Las entradas y salida ahora son números y cada conexión de entrada está asociada con un peso.\n",
    "\n",
    "La TLU calcula la suma ponderada de sus entradas:\n",
    "\n",
    "$$z = w_1x_1 + w_2x_2 + \\cdots + w_nx_n = \\boldsymbol{x}^T \\boldsymbol{w}$$\n",
    "\n",
    "Después se aplica una función escalón para determinar la salida de la neurona:\n",
    "\n",
    "$$h_w(\\boldsymbol{x}) = \\Theta(z)$$\n",
    "\n",
    "![](figures_intro_rna/fig10-4.png)\n",
    "\n",
    "Típicamente se usa la función escalón de Heaviside. A veces se usa la función de signo:\n",
    "\n",
    "$\\Theta(z) = \\begin{cases} 0 & z < 0 \\\\ 1 & z \\geq 0 \\end{cases}$\n",
    "\n",
    "$\\text{sgn}(z) = \\begin{cases} -1 & z < 0 \\\\ 0 & z = 0 \\\\ +1 & z > 0 \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-advancement",
   "metadata": {},
   "source": [
    "Un **perceptrón** está compuesto de una capa de TLUs, con cada TLU conectada a todas las entradas.\n",
    "\n",
    "Cuando todas las neuronas en una capa están conectadas a todas las neuronas en la capa anterior, se llama una **capa totalmente conectada** o una **capa densa**.\n",
    "\n",
    "Para enfatizar que todas las entradas van a todas las TLU, es común dibujar una capa de neuronas de entrada, que solamente pasan sus entradas a la próxima capa (*passthrough neurons*). Todas las neuronas de entrada forman la **capa de entrada**.\n",
    "\n",
    "Típicamente un *bias feature* está agregado ($x_0 = 1$) usando una neurona de *bias* que siempre tiene salida igual a $1$.\n",
    "\n",
    "![](figures_intro_rna/fig10-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-drawing",
   "metadata": {},
   "source": [
    "El perceptrón arriba puede clasificar instancias en $3$ clases binarias diferentes simultaneamente, así que es un clasificador de *multioutput*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-glory",
   "metadata": {},
   "source": [
    "Se puede determinar las salidas de una capa densa (totalmente conectada) usando:\n",
    "\n",
    "$$h_{\\boldsymbol{W},\\boldsymbol{b}}(\\boldsymbol{X}) = \\phi(\\boldsymbol{X}\\boldsymbol{W}+\\boldsymbol{b})$$\n",
    "\n",
    "* $\\boldsymbol{X}$ es una matriz de *features* de entrada, una fila por instancia, una columna por *feature*.\n",
    "* $\\boldsymbol{W}$ es una matriz de pesos, que contiene todos los pesos de las conexiones aparte de las conexiones con la neurona de *bias*. Tiene una fila por neurona de entrada, y una columna por neurona TLU.\n",
    "* El vector de *bias* $\\boldsymbol{b}$ contiene todos los pesos de las conexiones entre la neurona de *bias* y las neuronas TLU. Hay un término de *bias* por neurona TLU.\n",
    "* La función $\\phi$ se llama la **función de activación**. En el caso de una TLU, es una función escalón."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-correlation",
   "metadata": {},
   "source": [
    "##### Entrenamiento de un perceptrón\n",
    "\n",
    "Rosenblatt propuso la **regla de Hebb** para entrenar un perceptrón. Motivada por el comportamiento de neuronas biológicas, la regla de Hebb aumenta el peso de una conexión entre dos neuronas cuando ambas neuronas tienen la misma salida.\n",
    "\n",
    "De hecho, un perceptrón ocupa una variante de esta regla que toma en cuenta el error de la red: hay refuerza de conexiones que ayudan en reducir el error.\n",
    "\n",
    "La red recibe las instancias de entrenamiento una a la vez y realiza una predicción (hay una salida). Para cada neurona de salida que produce una predicción incorrecta, hay refuerza de los pesos de conexiones de entradas que habrían contribuido a una predicción correcta.\n",
    "\n",
    "$$w_{i,j}^{(t+1)} = w_{i,j}^{(t)} + \\eta \\left( y_j - \\hat{y}_j^{(t)} \\right) x_i$$\n",
    "\n",
    "* $w_{i,j}$ es el peso de la conexión entre la $i$-esima neurona de entrada y la $j$-esima neurona de salida.\n",
    "* $x_i$ es el $i$-esima valor de entrada de la instancia de entrenamiento actual.\n",
    "* $\\hat{y}_j$ es la salida de la $j$-esima neurona de salida, para la instancia de entrenamiento actual.\n",
    "* $y_j$ es la salida objetivo de la $j$-esima neurona de salida, para la instancia de entrenamiento actual.\n",
    "* $\\eta$ es la taza de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-courtesy",
   "metadata": {},
   "source": [
    "El límite de decisión de cada neurona de salida es lineal, así que un perceptrón no puede aprender un patrón complejo.\n",
    "\n",
    "Si las instancias están linealmente separables, Rosenblatt demostró que el algoritmo converge a una solución (**teorema de convergencia del Perceptrón**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-garden",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
